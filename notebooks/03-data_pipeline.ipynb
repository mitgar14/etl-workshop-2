{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Workshop #2**\n",
    "\n",
    "### *Data Pipeline - `spotify` and `the_grammy_awards` dataset*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Setting the project directory***\n",
    "This script attempts to change the current working directory to the specified path.\n",
    "If the directory change fails due to the directory not being found, it prints a message indicating that the user is already in the correct directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    os.chdir(\"../../Workshop #2\")\n",
    "except FileNotFoundError:\n",
    "    print(\"You are already in the correct directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Importing dependencies***\n",
    "\n",
    "**Modules for extraction:**\n",
    "* **src.extract.spotify_extract**\n",
    "* **src.extract.grammys_extract**: uses `src.database.db_operations`\n",
    "\n",
    "**Modules for transformation:**\n",
    "* **src.transform.spotify_transform**\n",
    "* **src.transform.grammys_transform**\n",
    "\n",
    "**Modules for merge:**\n",
    "* **src.transform.merge**\n",
    "\n",
    "**Modules for load:**\n",
    "* **src.load_and_store.load**\n",
    "\n",
    "---\n",
    "\n",
    "**For this environment we are using:**\n",
    "* ***Pandas*** >= 2.2.2\n",
    "\n",
    "**From the `src.database.db_operations` module, we are also using:**\n",
    "* ***SQLAlchemy*** >= 2.0.32\n",
    "    * *SQLAlchemy Utils* >= 0.41.2\n",
    "* ***python-dotenv*** >= 1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.extract.spotify_extract import extracting_spotify_data\n",
    "from src.extract.grammys_extract import extracting_grammys_data\n",
    "\n",
    "from src.transform.spotify_transform import transforming_spotify_data\n",
    "from src.transform.grammys_transform import transforming_grammys_data\n",
    "from src.transform.merge import merging_datasets\n",
    "\n",
    "from src.load_and_store.load import loading_clean_data\n",
    "from src.load_and_store.store import store_merged_data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Extracting the data***\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Spotify dataset**\n",
    "In this section we extract the CSV using the `spotify_extract` module functions. With the use of these functions we can further synthesize our ETL process, and they will be very useful for when we create the tasks using Apache Airflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_data = extracting_spotify_data(\"./data/raw/spotify_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Grammys dataset**\n",
    "\n",
    "The extraction process from the PostgreSQL database is performed from the `grammys_extract` module, facilitating the generation of logs in our ETL process. There is no need to create or dispose the connection engine from the notebook, as this process is already done in the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammys_data = extracting_grammys_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammys_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Transforming the data***\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Spotify transformations*\n",
    "  \n",
    "- Created a `transforming_spotify_data` function to clean and transform the Spotify DataFrame by:\n",
    "\n",
    "  - Removing unnecessary columns (e.g., `\"Unnamed: 0\"`).\n",
    "\n",
    "  - Eliminating null values and resetting the DataFrame index.\n",
    "\n",
    "  - Removing duplicates through several steps:\n",
    "    - Dropped exact duplicate rows.\n",
    "    - Removed duplicates based on the `\"track_id\"` column.\n",
    "    - Mapped detailed genres to broader categories using a predefined genre mapping dictionary.\n",
    "    - Dropped duplicates based on song names and artists, keeping the most popular entries.\n",
    "\n",
    "  - Generated new columns for enhanced data analysis:\n",
    "    - **`duration_min`**: Converted song duration from milliseconds to minutes.\n",
    "    - **`duration_category`**: Categorized songs based on their duration.\n",
    "    - **`popularity_category`**: Categorized songs based on their popularity scores.\n",
    "    - **`track_mood`**: Identified the mood of songs using valence scores.\n",
    "    - **`live_performance`**: Flagged songs with a high likelihood of being live performances.\n",
    "\n",
    "  - Dropped irrelevant columns to streamline the dataset (e.g., `\"loudness\"`, `\"mode\"`, `\"tempo\"`).\n",
    "  \n",
    "  - Included logging statements to document the cleaning and transformation process, as well as to catch any potential errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_df = transforming_spotify_data(spotify_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Grammys Transformations*\n",
    "\n",
    "- Created a `transforming_grammys_data` function to clean and transform the Grammy Awards DataFrame by:\n",
    "\n",
    "  - Renaming the column `winner` to `is_nominated`.\n",
    "\n",
    "  - Dropping unnecessary columns (e.g., `published_at`, `updated_at`, `img`).\n",
    "\n",
    "  - Removing rows with null values in `nominee`.\n",
    "\n",
    "  - Handling cases where both `artist` and `workers` are null:\n",
    "    - Filtered out specific categories listed in the `categories` list.\n",
    "    - For the remaining rows, filled `artist` with the value from `nominee`.\n",
    "\n",
    "  - Populating the `artist` column by applying several functions:\n",
    "    - **`extract_artist`**: Extracted artist names within parentheses from the `workers` column.\n",
    "    - **`move_workers_to_artist`**: Moved data from `workers` to `artist` if `artist` is null and `workers` doesn't contain semicolons or commas.\n",
    "    - **`extract_artists_before_semicolon`**: Extracted artist names before semicolons in `workers`, excluding any roles of interest.\n",
    "    - **`extract_roles_based_on_interest`**: Extracted names associated with specific roles defined in the `roles_of_interest` list from `workers`.\n",
    "\n",
    "  - Dropped rows with null values in `artist`.\n",
    "\n",
    "  - Replaced certain values in the `artist` column (e.g., changing `(Various Artists)` to `Various Artists`).\n",
    "\n",
    "  - Dropped the `workers` column as it was no longer needed.\n",
    "\n",
    "  - Included logging statements to document the cleaning and transformation process, as well as to catch any potential errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammys_df = transforming_grammys_data(grammys_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammys_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Merging the data***\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Merging Spotify and Grammys Datasets*\n",
    "\n",
    "- Created a `merge_datasets` function to combine the Spotify and Grammy Awards DataFrames by:\n",
    "\n",
    "  - **Cleaning key columns for accurate merging**:\n",
    "    - Converted the `track_name` column in the Spotify DataFrame to lowercase and stripped whitespace, creating a new column `track_name_clean`.\n",
    "    - Converted the `nominee` column in the Grammys DataFrame to lowercase and stripped whitespace, creating a new column `nominee_clean`.\n",
    "\n",
    "  - **Merging the datasets**:\n",
    "    - Performed a left join on the cleaned columns `track_name_clean` and `nominee_clean` to merge the DataFrames.\n",
    "    - Used suffixes to differentiate overlapping columns, appending `_grammys` to columns from the Grammys DataFrame when necessary.\n",
    "\n",
    "  - **Handling missing values**:\n",
    "    - Filled null values in the `title` and `category` columns with `\"Not applicable\"`.\n",
    "    - Filled null values in the `is_nominated` column with `False`.\n",
    "\n",
    "  - **Dropping unnecessary columns**:\n",
    "    - Removed columns that were no longer needed after the merge, such as `\"year\"`, `\"artist\"`, `\"nominee\"`, `\"nominee_clean\"`, and `\"track_name_clean\"`.\n",
    "\n",
    "- Included helper functions to streamline the data processing:\n",
    "\n",
    "  - **`fill_null_values`**: Filled null values in specified columns with a given value to ensure data completeness.\n",
    "\n",
    "  - **`drop_columns`**: Dropped specified columns from the DataFrame to eliminate redundancy and maintain a clean dataset.\n",
    "\n",
    "- **Used logging throughout** the process to monitor the merging steps and catch any potential issues, enhancing traceability and debugging capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merging_datasets(spotify_df, grammys_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.query(\"is_nominated == True\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing the merge we can notice a change in the number of rows: an increase of approximately 600 records. Why is this happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the number of unique values for *track_id* we see that it is still the same as the Spotify dataset. However, this ensures that suddenly the rise in the number of rows is due to an increase in duplicates within our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"track_id\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to group the duplicated songs in order to know what could condition this increase in the number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_counts = (merged_df\n",
    "                     .groupby([\"track_id\", \"track_name\", \"artists\", \"album_name\"])\n",
    "                     .size()\n",
    "                     .reset_index(name=\"duplicate_count\")\n",
    "                     .sort_values(by=\"duplicate_count\", ascending=False))\n",
    "\n",
    "duplicated_values = duplicated_counts.query(\"duplicate_count > 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking at the results for this song, we can see that:**\n",
    "\n",
    "* The fact that a record starts repeating several times may be due to the fact that the song has several nominations.\n",
    "\n",
    "* **However, the merge criteria can also lead to confusion**: here we find that Elvis is nominated for *Best R&B Vocal Performance, Female*. Elvis is so ubiquitous that he ended up being nominated in a female category.\n",
    "\n",
    "    * This is caused by the similarity of the values in *track_name*. In order to avoid it, we should add one more criterion to the merge process, but it would imply a much more careful and specific cleanup in the *artist* columns of both datasets.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.query(\"track_id == '0gaZi6J3Pk7FG7GNMHsK5o'\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Loading the data***\n",
    "---\n",
    "\n",
    "The `loading_clean_data` function loads a Pandas DataFrame into a database. It logs the process, creates an engine, and tries to load data using `load_clean_data`. If successful, it logs a success message, otherwise logs any errors. The `load_clean_data` function checks if the table exists, creates it if needed, and loads the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading_clean_data(merged_df, \"merged_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Storing the data***\n",
    "---\n",
    "\n",
    "The `store_merged_data` function uploads a merged DataFrame as a CSV file to Google Drive. It authenticates the drive, converts the DataFrame to CSV format, and creates a new file in the specified Google Drive folder. The content is uploaded, and a success message is logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_merged_data(\"merged_data\", merged_df, \"1x3tS43kSxC2oKhq7xCiJFzXqGerGvcy7\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop_#2_-_virtualenv_for_jupyter-YTnIm7kx-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
